{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDClassifier, Lasso,RidgeCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve, KFold\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_absolute_percentage_error, RocCurveDisplay, mean_squared_log_error,  accuracy_score,precision_score,recall_score,f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, RobustScaler, StandardScaler, FunctionTransformer, LabelEncoder, OneHotEncoder, Binarizer, OrdinalEncoder, MaxAbsScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, RFECV \n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor, ExtraTreesRegressor, VotingClassifier, VotingRegressor,StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# from pycaret.regression import *\n",
    "import xgboost\n",
    "from os import stat\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from contextlib import closing\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from datetime import datetime \n",
    "import lightgbm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather(x):\n",
    "    if x == 1:\n",
    "        x = 4\n",
    "    elif x == 2:\n",
    "        x = 3\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_season(x):\n",
    "    if x == 1:\n",
    "        x = 3\n",
    "    elif x == 2:\n",
    "        x = 4\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les transformations : \n",
    "\n",
    "Transformation des variables weather & season\n",
    "(1 à 4) => 1 les mauvais jours et 4 les meilleurs\n",
    "\n",
    "\n",
    "Ajout de data avec 0 count aux températures extrême pour donner une info type catastrophe naturelle. \n",
    "\n",
    "Pour insister sur le fait que le pic de fréquentation est aux alentours des 30 degrès et n'augmentera pas s'il fait \"trop\" chaud\n",
    "\n",
    "\n",
    "Transformation colonne datetime en dayofyear, hour et year. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les transformations \n",
    "train_data_all =  pd.read_csv('../data/train.csv')\n",
    "train_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-17 08:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 75, 'atemp' : 72, 'humidity' : 2, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-07-15 09:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : 96, 'atemp' : 88, 'humidity' : 2, 'windspeed' : 500.0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-10 11:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 51, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-12 19:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 4, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-08 18:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 9, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-24 17:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -110, 'atemp' : -108, 'humidity' : 25, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-25 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 69, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : -500, 'atemp' : -55, 'humidity' : 0, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-07-21 15:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 2, 'temp' : 75, 'atemp' : 72.5, 'humidity' : 41, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-04 13:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 55, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-05 14:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 25, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-28 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 39, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 09:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : -110, 'atemp' : -108, 'humidity' : 10, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-27 12:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 57, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-01-02 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -500, 'atemp' : -55, 'humidity' : 84, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "\n",
    "train_data_all['day'] = pd.to_datetime(train_data_all['datetime']).dt.dayofyear\n",
    "train_data_all['hour'] = pd.to_datetime(train_data_all['datetime']).dt.hour\n",
    "train_data_all['year'] = pd.to_datetime(train_data_all['datetime']).dt.year\n",
    "\n",
    "train_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "for i in range(0, len(train_data_all.iloc[:,0])) :\n",
    "    train_data_all.iloc[i,0] = transform_season(int(train_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(train_data_all.iloc[:,3])) :\n",
    "    train_data_all.iloc[i,3] = transform_weather(int(train_data_all.iloc[i,3] ))\n",
    "\n",
    "\n",
    "train_data_all = train_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "augmentation = np.linspace(0, (144 * 100 / 230) *  train_data_all.shape[0] , train_data_all.shape[0])\n",
    "\n",
    "# train_data_all['augmentation'] = 0 \n",
    "# train_data_all['augmentation'] = augmentation # Etape suivante : Il faut order par day avant de drop datetime pour que ce soit cohérent \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_all_X = train_data_all.drop('count', axis = 1)\n",
    "train_data_all_y = train_data_all['count']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data_all =  pd.read_csv('../data/test.csv')\n",
    "test_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data_all['day'] = pd.to_datetime(test_data_all['datetime']).dt.dayofyear\n",
    "test_data_all['hour'] = pd.to_datetime(test_data_all['datetime']).dt.hour\n",
    "test_data_all['year'] = pd.to_datetime(test_data_all['datetime']).dt.year\n",
    "test_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(0, len(test_data_all.iloc[:,0])) :\n",
    "    test_data_all.iloc[i,0] = transform_season(int(test_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(test_data_all.iloc[:,3])) :\n",
    "    test_data_all.iloc[i,3] = transform_weather(int(test_data_all.iloc[i,3] ))\n",
    "\n",
    "# augmentation = np.linspace(0, (144 * 100 / 230) *  test_data_all.shape[0] / 100 , test_data_all.shape[0])\n",
    "# test_data_all['augmentation'] = augmentation\n",
    "\n",
    "\n",
    "test_data_all = test_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "test_data_all_X = test_data_all.drop('count', axis = 1)\n",
    "test_data_all_y = test_data_all['count']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise deux pipelines, l'une pour traité les valeurs numériques et l'autres pour les valeurs catégoriques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:01:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.986720359083927\n",
      "test data 0.8967965326020695\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_xgboost = make_pipeline(preprocessor,  xgb)\n",
    "model_xgboost.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_xgboost.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_xgboost.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9614442590972704\n",
      "test 0.9190960274167893\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(missing_values=np.nan), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9521333672398784\n",
      "test 0.9051328597939277\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(  SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_2)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "train 0.9522756520038445\n",
      "test 0.9118832634446215\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model n2 lgbm : \n",
    "# Preprocessing et model lgbm :0.9118832634446215 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor,  model_lgbm_3)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9588957920387866\n",
      "test 0.9074659738499581\n"
     ]
    }
   ],
   "source": [
    "# colsample_bytree => Pourcentage de features utilisé pour chaque arbre réduit a 46% et resultat pas loin de 73% + modif learning rate \n",
    "# Preprocessing et model lgbm : 0.9064545094483808(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_4)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele => Voting Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9140491767414458"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.917337454042215 (2s4)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_voting = make_pipeline(preprocessor, VC)\n",
    "\n",
    "model_voting.fit(train_data_all_X, train_data_all_y)\n",
    "model_voting.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model de stacking => "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "mean abs error 39.64159024921\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_3)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)\n",
    "y_pred = model_stack.predict(test_data_all_X)\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244984606061998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:00:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244984606061998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les modèles avec une fonction logarythme donner de bons score mais elle pose un problème de prédictions avec les valeurs négatives. \n",
    "\n",
    "# On a donc décidé de retiré la fonction logarythme et d'optimiser les paramètres pour rehausser le score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:01:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.986720359083927\n",
      "test data 0.8967965326020695\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_xgboost = make_pipeline(preprocessor,  xgb)\n",
    "model_xgboost.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_xgboost.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_xgboost.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9521333672398784\n",
      "test 0.9051328597939277\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(  SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_2)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models \n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test d'optimisation avec Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "def objective(trial):\n",
    "    import xgboost as xgb \n",
    "    dtrain = xgb.DMatrix(train_data_all_X, label=train_data_all_y)\n",
    "    dtest = xgb.DMatrix(test_data_all_X, label=test_data_all_y)\n",
    "\n",
    "    param = {\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.0, 1.0),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-8, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 0, 50),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 2.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 2.0),\n",
    "        \"eval_metric\": \"mae\",\n",
    "        \"random_state\" : 2\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-mae\")\n",
    "    best_model = xgb.train(param, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n",
    "    preds = best_model.predict(dtest)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = mean_absolute_error(test_data_all_y, pred_labels)\n",
    "    # accuracy = { 'r2_score' : round(r2_score(test_data_all_y, pred_labels), 4), 'mse' :  round(mean_squared_error(test_data_all_y, pred_labels ),4), 'median abs err' :  round(median_absolute_error(test_data_all_y, pred_labels),4), 'mean abs err' :  round(mean_absolute_error(test_data_all_y, pred_labels),4)}\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:08,572]\u001b[0m A new study created in memory with name: no-name-315528cd-905d-445d-a67e-446c70270384\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:182.12363\n",
      "[1]\tvalidation-mae:172.77174\n",
      "[2]\tvalidation-mae:165.73238\n",
      "[3]\tvalidation-mae:156.70548\n",
      "[4]\tvalidation-mae:154.19252\n",
      "[5]\tvalidation-mae:153.07251\n",
      "[6]\tvalidation-mae:152.89870\n",
      "[7]\tvalidation-mae:152.94453\n",
      "[8]\tvalidation-mae:151.28181\n",
      "[9]\tvalidation-mae:151.28134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:08,801]\u001b[0m Trial 0 finished with value: 151.2803071859572 and parameters: {'booster': 'dart', 'colsample_bytree': 0.06926902275121016, 'eta': 0.883165156722046, 'gamma': 0.724122320905529, 'max_depth': 6, 'max_leaves': 46, 'alpha': 7.336828175554749e-07, 'lambda': 0.001455838076469405}. Best is trial 0 with value: 151.2803071859572.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:75.95242\n",
      "[1]\tvalidation-mae:65.41508\n",
      "[2]\tvalidation-mae:60.65850\n",
      "[3]\tvalidation-mae:60.71726\n",
      "[4]\tvalidation-mae:60.25446\n",
      "[5]\tvalidation-mae:60.43544\n",
      "[6]\tvalidation-mae:61.96095\n",
      "[7]\tvalidation-mae:62.50245\n",
      "[8]\tvalidation-mae:62.63272\n",
      "[9]\tvalidation-mae:65.38998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:09,055]\u001b[0m Trial 1 finished with value: 65.38507953922107 and parameters: {'booster': 'gbtree', 'colsample_bytree': 0.9089659669004314, 'eta': 0.9749967096614847, 'gamma': 0.9193238816097603, 'max_depth': 7, 'max_leaves': 35, 'alpha': 1.0064876485480154e-06, 'lambda': 0.0014320554373507778}. Best is trial 0 with value: 151.2803071859572.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:178.35680\n",
      "[1]\tvalidation-mae:164.03906\n",
      "[2]\tvalidation-mae:159.43570\n",
      "[3]\tvalidation-mae:155.48962\n",
      "[4]\tvalidation-mae:151.48408\n",
      "[5]\tvalidation-mae:147.70956\n",
      "[6]\tvalidation-mae:144.45575\n",
      "[7]\tvalidation-mae:141.84444\n",
      "[8]\tvalidation-mae:139.86281\n",
      "[9]\tvalidation-mae:138.48119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:09,224]\u001b[0m Trial 2 finished with value: 138.4766867800329 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.3922940199255336, 'eta': 0.9098781358008561, 'gamma': 0.06871360048677093, 'max_depth': 1, 'max_leaves': 36, 'alpha': 0.004776414005737868, 'lambda': 0.08312748449467988}. Best is trial 0 with value: 151.2803071859572.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:207.22380\n",
      "[1]\tvalidation-mae:185.39681\n",
      "[2]\tvalidation-mae:151.37370\n",
      "[3]\tvalidation-mae:142.80370\n",
      "[4]\tvalidation-mae:117.87153\n",
      "[5]\tvalidation-mae:110.45898\n",
      "[6]\tvalidation-mae:102.95335\n",
      "[7]\tvalidation-mae:98.40242\n",
      "[8]\tvalidation-mae:95.32945\n",
      "[9]\tvalidation-mae:93.12370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:09,389]\u001b[0m Trial 3 finished with value: 93.11738891936369 and parameters: {'booster': 'gbtree', 'colsample_bytree': 0.4685990591148401, 'eta': 0.3840129949973074, 'gamma': 0.5857722604747481, 'max_depth': 3, 'max_leaves': 31, 'alpha': 4.508265786254668e-05, 'lambda': 0.027262249701110606}. Best is trial 0 with value: 151.2803071859572.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:195.29256\n",
      "[1]\tvalidation-mae:172.95143\n",
      "[2]\tvalidation-mae:131.22519\n",
      "[3]\tvalidation-mae:127.98367\n",
      "[4]\tvalidation-mae:126.98946\n",
      "[5]\tvalidation-mae:126.46380\n",
      "[6]\tvalidation-mae:126.49191\n",
      "[7]\tvalidation-mae:126.45757\n",
      "[8]\tvalidation-mae:117.55688\n",
      "[9]\tvalidation-mae:117.52764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:09,575]\u001b[0m Trial 4 finished with value: 117.52605595172793 and parameters: {'booster': 'gbtree', 'colsample_bytree': 0.09097363482622334, 'eta': 0.6306028578541997, 'gamma': 0.5514201645587946, 'max_depth': 5, 'max_leaves': 45, 'alpha': 0.042330029123346234, 'lambda': 0.00020659160361112585}. Best is trial 0 with value: 151.2803071859572.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:09,619]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:217.36824\n",
      "[1]\tvalidation-mae:181.45000\n",
      "[2]\tvalidation-mae:162.06584\n",
      "[3]\tvalidation-mae:146.95767\n",
      "[4]\tvalidation-mae:130.60239\n",
      "[5]\tvalidation-mae:122.56697\n",
      "[6]\tvalidation-mae:114.21401\n",
      "[7]\tvalidation-mae:108.66544\n",
      "[8]\tvalidation-mae:105.19201\n",
      "[9]\tvalidation-mae:102.23010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:09,807]\u001b[0m Trial 6 finished with value: 102.20899616017553 and parameters: {'booster': 'dart', 'colsample_bytree': 0.7053644438113286, 'eta': 0.22876087168711562, 'gamma': 0.6645785548046311, 'max_depth': 3, 'max_leaves': 28, 'alpha': 9.824122655430147e-05, 'lambda': 1.8432701735819646e-05}. Best is trial 0 with value: 151.2803071859572.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:09,841]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:11:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:09,973]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:09] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:181.39287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:10,011]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:241.94527\n",
      "[1]\tvalidation-mae:231.45219\n",
      "[2]\tvalidation-mae:219.66995\n",
      "[3]\tvalidation-mae:206.89737\n",
      "[4]\tvalidation-mae:199.37566\n",
      "[5]\tvalidation-mae:192.19443\n",
      "[6]\tvalidation-mae:186.71809\n",
      "[7]\tvalidation-mae:182.63055\n",
      "[8]\tvalidation-mae:178.33142\n",
      "[9]\tvalidation-mae:174.93503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:10,245]\u001b[0m Trial 10 finished with value: 174.9292375205705 and parameters: {'booster': 'dart', 'colsample_bytree': 0.20859025935072043, 'eta': 0.0934832702646371, 'gamma': 0.7621491606437919, 'max_depth': 9, 'max_leaves': 14, 'alpha': 4.029676311704897e-06, 'lambda': 1.4342836039299758e-08}. Best is trial 10 with value: 174.9292375205705.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.76779\n",
      "[1]\tvalidation-mae:254.48163\n",
      "[2]\tvalidation-mae:254.08040\n",
      "[3]\tvalidation-mae:253.76941\n",
      "[4]\tvalidation-mae:253.45940\n",
      "[5]\tvalidation-mae:253.10745\n",
      "[6]\tvalidation-mae:252.81297\n",
      "[7]\tvalidation-mae:252.52852\n",
      "[8]\tvalidation-mae:252.25603\n",
      "[9]\tvalidation-mae:251.97250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:10,471]\u001b[0m Trial 11 finished with value: 251.97805814591334 and parameters: {'booster': 'dart', 'colsample_bytree': 0.21548199630433704, 'eta': 0.0017744271554746144, 'gamma': 0.7781986950973909, 'max_depth': 9, 'max_leaves': 13, 'alpha': 4.881429362595222e-06, 'lambda': 4.041531956291691e-08}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:247.79181\n",
      "[1]\tvalidation-mae:241.47040\n",
      "[2]\tvalidation-mae:233.98978\n",
      "[3]\tvalidation-mae:226.66899\n",
      "[4]\tvalidation-mae:221.48585\n",
      "[5]\tvalidation-mae:216.08485\n",
      "[6]\tvalidation-mae:211.83777\n",
      "[7]\tvalidation-mae:208.33386\n",
      "[8]\tvalidation-mae:204.83244\n",
      "[9]\tvalidation-mae:201.76280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:10,760]\u001b[0m Trial 12 finished with value: 201.7657707076248 and parameters: {'booster': 'dart', 'colsample_bytree': 0.23291773261434517, 'eta': 0.04795993061667221, 'gamma': 0.8213074831051244, 'max_depth': 9, 'max_leaves': 14, 'alpha': 9.559380917901847e-06, 'lambda': 1.1252520215867039e-08}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:248.42026\n",
      "[1]\tvalidation-mae:242.56398\n",
      "[2]\tvalidation-mae:235.60234\n",
      "[3]\tvalidation-mae:228.88545\n",
      "[4]\tvalidation-mae:224.02571\n",
      "[5]\tvalidation-mae:218.93558\n",
      "[6]\tvalidation-mae:214.91823\n",
      "[7]\tvalidation-mae:211.53209\n",
      "[8]\tvalidation-mae:208.17351\n",
      "[9]\tvalidation-mae:205.17558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:10,999]\u001b[0m Trial 13 finished with value: 205.16950082281951 and parameters: {'booster': 'dart', 'colsample_bytree': 0.2625284282342054, 'eta': 0.04349934663795158, 'gamma': 0.842626616963964, 'max_depth': 9, 'max_leaves': 16, 'alpha': 0.0007299802887841125, 'lambda': 1.707365013526199e-08}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.03912\n",
      "[1]\tvalidation-mae:253.03807\n",
      "[2]\tvalidation-mae:251.99056\n",
      "[3]\tvalidation-mae:251.00195\n",
      "[4]\tvalidation-mae:249.46498\n",
      "[5]\tvalidation-mae:248.29610\n",
      "[6]\tvalidation-mae:247.29677\n",
      "[7]\tvalidation-mae:246.33852\n",
      "[8]\tvalidation-mae:245.38773\n",
      "[9]\tvalidation-mae:244.41818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:11,286]\u001b[0m Trial 14 finished with value: 244.41634668129456 and parameters: {'booster': 'dart', 'colsample_bytree': 0.6164191719138364, 'eta': 0.005888919308624785, 'gamma': 0.9592077642128655, 'max_depth': 8, 'max_leaves': 15, 'alpha': 1.3610394349023947, 'lambda': 3.386003923854695e-07}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:219.14899\n",
      "[1]\tvalidation-mae:191.18417\n",
      "[2]\tvalidation-mae:170.41475\n",
      "[3]\tvalidation-mae:154.47899\n",
      "[4]\tvalidation-mae:131.56419\n",
      "[5]\tvalidation-mae:114.93405\n",
      "[6]\tvalidation-mae:106.66414\n",
      "[7]\tvalidation-mae:99.43175\n",
      "[8]\tvalidation-mae:95.74556\n",
      "[9]\tvalidation-mae:93.03888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:11,534]\u001b[0m Trial 15 finished with value: 93.04168952276467 and parameters: {'booster': 'dart', 'colsample_bytree': 0.6243331071219823, 'eta': 0.2094385502298297, 'gamma': 0.9456213538291879, 'max_depth': 8, 'max_leaves': 2, 'alpha': 0.6335616922008092, 'lambda': 3.15265584729006e-07}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:219.98843\n",
      "[1]\tvalidation-mae:184.41211\n",
      "[2]\tvalidation-mae:163.98126\n",
      "[3]\tvalidation-mae:147.95931\n",
      "[4]\tvalidation-mae:126.98985\n",
      "[5]\tvalidation-mae:112.01759\n",
      "[6]\tvalidation-mae:102.82375\n",
      "[7]\tvalidation-mae:95.31818\n",
      "[8]\tvalidation-mae:89.96648\n",
      "[9]\tvalidation-mae:86.98853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:11,791]\u001b[0m Trial 16 finished with value: 86.9884805266045 and parameters: {'booster': 'dart', 'colsample_bytree': 0.6531928237160768, 'eta': 0.20570421812036166, 'gamma': 0.995160380443752, 'max_depth': 7, 'max_leaves': 21, 'alpha': 0.16131209790284526, 'lambda': 4.6772864000870534e-07}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:203.80107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:11,864]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:143.24156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:11,924]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:235.98483\n",
      "[1]\tvalidation-mae:222.60658\n",
      "[2]\tvalidation-mae:211.68642\n",
      "[3]\tvalidation-mae:196.32336\n",
      "[4]\tvalidation-mae:175.07155\n",
      "[5]\tvalidation-mae:169.83881\n",
      "[6]\tvalidation-mae:158.23164\n",
      "[7]\tvalidation-mae:148.76592\n",
      "[8]\tvalidation-mae:143.93356\n",
      "[9]\tvalidation-mae:140.12972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:12,211]\u001b[0m Trial 19 finished with value: 140.1519473395502 and parameters: {'booster': 'dart', 'colsample_bytree': 0.5169613427445211, 'eta': 0.1286792405899715, 'gamma': 0.4513897154975473, 'max_depth': 6, 'max_leaves': 8, 'alpha': 0.0004380549255173246, 'lambda': 1.3532646524310452e-06}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:252.14282\n",
      "[1]\tvalidation-mae:249.30893\n",
      "[2]\tvalidation-mae:246.37860\n",
      "[3]\tvalidation-mae:243.22498\n",
      "[4]\tvalidation-mae:239.68634\n",
      "[5]\tvalidation-mae:236.59656\n",
      "[6]\tvalidation-mae:233.69734\n",
      "[7]\tvalidation-mae:230.87045\n",
      "[8]\tvalidation-mae:228.74793\n",
      "[9]\tvalidation-mae:226.69754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:12,640]\u001b[0m Trial 20 finished with value: 226.70707624794295 and parameters: {'booster': 'dart', 'colsample_bytree': 0.36509647665390826, 'eta': 0.018988441065488405, 'gamma': 0.7392278061737684, 'max_depth': 4, 'max_leaves': 19, 'alpha': 1.8620496387071632e-05, 'lambda': 1.3374089828644766}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.74734\n",
      "[1]\tvalidation-mae:254.45288\n",
      "[2]\tvalidation-mae:254.06751\n",
      "[3]\tvalidation-mae:253.73364\n",
      "[4]\tvalidation-mae:253.35275\n",
      "[5]\tvalidation-mae:252.99802\n",
      "[6]\tvalidation-mae:252.68532\n",
      "[7]\tvalidation-mae:252.37428\n",
      "[8]\tvalidation-mae:252.07803\n",
      "[9]\tvalidation-mae:251.78154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:12,965]\u001b[0m Trial 21 finished with value: 251.74382885353813 and parameters: {'booster': 'dart', 'colsample_bytree': 0.4079495008075114, 'eta': 0.0019011971820242978, 'gamma': 0.7368162060446231, 'max_depth': 3, 'max_leaves': 18, 'alpha': 1.6930854226332067e-05, 'lambda': 0.862441609794628}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:226.45609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:13,025]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:224.20136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:13,086]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:217.15300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:13,147]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.30470\n",
      "[1]\tvalidation-mae:247.64224\n",
      "[2]\tvalidation-mae:243.79855\n",
      "[3]\tvalidation-mae:240.34013\n",
      "[4]\tvalidation-mae:234.90727\n",
      "[5]\tvalidation-mae:230.88272\n",
      "[6]\tvalidation-mae:227.51387\n",
      "[7]\tvalidation-mae:224.33491\n",
      "[8]\tvalidation-mae:221.44624\n",
      "[9]\tvalidation-mae:218.65190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:13,406]\u001b[0m Trial 25 finished with value: 218.66758091058693 and parameters: {'booster': 'dart', 'colsample_bytree': 0.5522664963084081, 'eta': 0.02186880312261459, 'gamma': 0.7805087124973423, 'max_depth': 6, 'max_leaves': 10, 'alpha': 0.00027008668053172814, 'lambda': 4.4757804626783305e-05}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:236.30238\n",
      "[1]\tvalidation-mae:223.36192\n",
      "[2]\tvalidation-mae:210.60606\n",
      "[3]\tvalidation-mae:195.08231\n",
      "[4]\tvalidation-mae:186.34935\n",
      "[5]\tvalidation-mae:178.40042\n",
      "[6]\tvalidation-mae:167.93808\n",
      "[7]\tvalidation-mae:157.78532\n",
      "[8]\tvalidation-mae:153.59067\n",
      "[9]\tvalidation-mae:149.89288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:13,757]\u001b[0m Trial 26 finished with value: 149.89358200767964 and parameters: {'booster': 'dart', 'colsample_bytree': 0.28189605025316, 'eta': 0.12405908733392618, 'gamma': 0.5756486428343788, 'max_depth': 8, 'max_leaves': 0, 'alpha': 1.990101477451556e-06, 'lambda': 2.1607755147484807e-06}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.56905\n",
      "[1]\tvalidation-mae:254.08173\n",
      "[2]\tvalidation-mae:253.39571\n",
      "[3]\tvalidation-mae:252.91380\n",
      "[4]\tvalidation-mae:252.44455\n",
      "[5]\tvalidation-mae:251.96904\n",
      "[6]\tvalidation-mae:251.49759\n",
      "[7]\tvalidation-mae:250.98637\n",
      "[8]\tvalidation-mae:250.52272\n",
      "[9]\tvalidation-mae:250.02446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:13,961]\u001b[0m Trial 27 finished with value: 249.996708721887 and parameters: {'booster': 'dart', 'colsample_bytree': 0.0017219775217401923, 'eta': 0.003014463272189012, 'gamma': 0.9894910448767965, 'max_depth': 4, 'max_leaves': 26, 'alpha': 1.827746088119112e-05, 'lambda': 5.649841424248235e-08}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:205.70387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:14,020]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:168.02702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:14,070]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:242.06531\n",
      "[1]\tvalidation-mae:232.24175\n",
      "[2]\tvalidation-mae:220.73413\n",
      "[3]\tvalidation-mae:213.48680\n",
      "[4]\tvalidation-mae:208.13690\n",
      "[5]\tvalidation-mae:202.93927\n",
      "[6]\tvalidation-mae:198.09045\n",
      "[7]\tvalidation-mae:194.14223\n",
      "[8]\tvalidation-mae:190.14143\n",
      "[9]\tvalidation-mae:187.12579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:14,272]\u001b[0m Trial 30 finished with value: 187.12452002194186 and parameters: {'booster': 'dart', 'colsample_bytree': 0.004320289044829145, 'eta': 0.09198872796463203, 'gamma': 0.3417318798834804, 'max_depth': 2, 'max_leaves': 5, 'alpha': 8.025031211421457e-06, 'lambda': 0.01989334722696186}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:253.57393\n",
      "[1]\tvalidation-mae:252.21265\n",
      "[2]\tvalidation-mae:250.58209\n",
      "[3]\tvalidation-mae:249.10570\n",
      "[4]\tvalidation-mae:247.69435\n",
      "[5]\tvalidation-mae:246.14865\n",
      "[6]\tvalidation-mae:244.73668\n",
      "[7]\tvalidation-mae:243.35358\n",
      "[8]\tvalidation-mae:242.20621\n",
      "[9]\tvalidation-mae:241.08231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:14,503]\u001b[0m Trial 31 finished with value: 241.0729566648382 and parameters: {'booster': 'dart', 'colsample_bytree': 0.328574684010296, 'eta': 0.008567143451154925, 'gamma': 0.999632318085586, 'max_depth': 5, 'max_leaves': 14, 'alpha': 8.080101547429462e-07, 'lambda': 4.2074551510143066e-08}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:224.35680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:14,563]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:246.72028\n",
      "[1]\tvalidation-mae:239.80113\n",
      "[2]\tvalidation-mae:231.54820\n",
      "[3]\tvalidation-mae:226.06439\n",
      "[4]\tvalidation-mae:221.65006\n",
      "[5]\tvalidation-mae:217.35609\n",
      "[6]\tvalidation-mae:213.42574\n",
      "[7]\tvalidation-mae:209.89848\n",
      "[8]\tvalidation-mae:206.38669\n",
      "[9]\tvalidation-mae:203.38475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:14,889]\u001b[0m Trial 33 finished with value: 203.39879319802523 and parameters: {'booster': 'dart', 'colsample_bytree': 0.180157124977102, 'eta': 0.05516542185838508, 'gamma': 0.7767753723434809, 'max_depth': 4, 'max_leaves': 39, 'alpha': 0.0001508350141346915, 'lambda': 4.672645971067518e-08}. Best is trial 11 with value: 251.97805814591334.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:255.00778\n",
      "[1]\tvalidation-mae:254.96805\n",
      "[2]\tvalidation-mae:254.91586\n",
      "[3]\tvalidation-mae:254.87027\n",
      "[4]\tvalidation-mae:254.81850\n",
      "[5]\tvalidation-mae:254.77023\n",
      "[6]\tvalidation-mae:254.72749\n",
      "[7]\tvalidation-mae:254.68510\n",
      "[8]\tvalidation-mae:254.64441\n",
      "[9]\tvalidation-mae:254.60350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:15,104]\u001b[0m Trial 34 finished with value: 254.54854635216677 and parameters: {'booster': 'dart', 'colsample_bytree': 0.4158466200732063, 'eta': 0.00025675381378648045, 'gamma': 0.9941895100340654, 'max_depth': 3, 'max_leaves': 12, 'alpha': 9.463978241232774e-07, 'lambda': 7.470523006997358e-07}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:242.88997\n",
      "[1]\tvalidation-mae:233.51335\n",
      "[2]\tvalidation-mae:219.65153\n",
      "[3]\tvalidation-mae:212.73587\n",
      "[4]\tvalidation-mae:196.48242\n",
      "[5]\tvalidation-mae:187.98080\n",
      "[6]\tvalidation-mae:181.04013\n",
      "[7]\tvalidation-mae:173.09088\n",
      "[8]\tvalidation-mae:167.59320\n",
      "[9]\tvalidation-mae:160.89252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:15,268]\u001b[0m Trial 35 finished with value: 160.87547997805814 and parameters: {'booster': 'gbtree', 'colsample_bytree': 0.4239907272298651, 'eta': 0.08140191396963603, 'gamma': 0.8775841156089813, 'max_depth': 3, 'max_leaves': 49, 'alpha': 1.3030173458386e-06, 'lambda': 0.0005328523451741736}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:15,329]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:241.95555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:15,395]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:15,441]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:15,495]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:234.33095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:15,553]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:252.94292\n",
      "[1]\tvalidation-mae:250.85213\n",
      "[2]\tvalidation-mae:248.54878\n",
      "[3]\tvalidation-mae:246.47839\n",
      "[4]\tvalidation-mae:243.45184\n",
      "[5]\tvalidation-mae:241.08714\n",
      "[6]\tvalidation-mae:239.18875\n",
      "[7]\tvalidation-mae:237.25497\n",
      "[8]\tvalidation-mae:235.36230\n",
      "[9]\tvalidation-mae:233.55272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:15,840]\u001b[0m Trial 41 finished with value: 233.5479978058146 and parameters: {'booster': 'dart', 'colsample_bytree': 0.5943358179806433, 'eta': 0.012400397889207456, 'gamma': 0.9934783786352295, 'max_depth': 9, 'max_leaves': 16, 'alpha': 0.09017716482016418, 'lambda': 2.584451848705901e-07}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.34085\n",
      "[1]\tvalidation-mae:253.69885\n",
      "[2]\tvalidation-mae:253.02803\n",
      "[3]\tvalidation-mae:252.33865\n",
      "[4]\tvalidation-mae:251.23053\n",
      "[5]\tvalidation-mae:250.91315\n",
      "[6]\tvalidation-mae:250.22621\n",
      "[7]\tvalidation-mae:249.54207\n",
      "[8]\tvalidation-mae:248.88083\n",
      "[9]\tvalidation-mae:248.26341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:16,225]\u001b[0m Trial 42 finished with value: 248.2709818979704 and parameters: {'booster': 'dart', 'colsample_bytree': 0.4692628138090748, 'eta': 0.004116760419547631, 'gamma': 0.9389392318532399, 'max_depth': 7, 'max_leaves': 12, 'alpha': 0.022754885896135595, 'lambda': 3.4852849850474715e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:244.51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:16,295]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:16,395]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:16,592]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:158.98373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:16,654]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:226.30231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:16,708]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.40717\n",
      "[1]\tvalidation-mae:253.90523\n",
      "[2]\tvalidation-mae:253.18642\n",
      "[3]\tvalidation-mae:252.63237\n",
      "[4]\tvalidation-mae:252.10678\n",
      "[5]\tvalidation-mae:251.61885\n",
      "[6]\tvalidation-mae:251.13458\n",
      "[7]\tvalidation-mae:250.64537\n",
      "[8]\tvalidation-mae:250.16197\n",
      "[9]\tvalidation-mae:249.69080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:17,032]\u001b[0m Trial 48 finished with value: 249.6769061985738 and parameters: {'booster': 'dart', 'colsample_bytree': 0.2088874934067175, 'eta': 0.0031646253840450097, 'gamma': 0.791556499059731, 'max_depth': 1, 'max_leaves': 18, 'alpha': 3.911573974357043e-05, 'lambda': 3.0521640629176967e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:226.27875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:17,100]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:17,156]\u001b[0m Trial 50 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:247.20300\n",
      "[1]\tvalidation-mae:239.86609\n",
      "[2]\tvalidation-mae:232.79781\n",
      "[3]\tvalidation-mae:226.16623\n",
      "[4]\tvalidation-mae:217.23418\n",
      "[5]\tvalidation-mae:211.47325\n",
      "[6]\tvalidation-mae:205.94653\n",
      "[7]\tvalidation-mae:200.72272\n",
      "[8]\tvalidation-mae:196.71957\n",
      "[9]\tvalidation-mae:193.05278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:17,526]\u001b[0m Trial 51 finished with value: 192.96599012616565 and parameters: {'booster': 'dart', 'colsample_bytree': 0.5582403077083584, 'eta': 0.04720834758383061, 'gamma': 0.8113683789953079, 'max_depth': 2, 'max_leaves': 17, 'alpha': 0.0014805733847787776, 'lambda': 7.184913797863593e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.80542\n",
      "[1]\tvalidation-mae:254.55009\n",
      "[2]\tvalidation-mae:254.27878\n",
      "[3]\tvalidation-mae:254.00038\n",
      "[4]\tvalidation-mae:253.68320\n",
      "[5]\tvalidation-mae:253.34621\n",
      "[6]\tvalidation-mae:253.07809\n",
      "[7]\tvalidation-mae:252.81082\n",
      "[8]\tvalidation-mae:252.56883\n",
      "[9]\tvalidation-mae:252.32796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:18,855]\u001b[0m Trial 52 finished with value: 252.34613274821723 and parameters: {'booster': 'dart', 'colsample_bytree': 0.36405994799470165, 'eta': 0.0015808569851585314, 'gamma': 0.9209932334496925, 'max_depth': 4, 'max_leaves': 9, 'alpha': 0.00026113039758456503, 'lambda': 2.362363969177556e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:239.92204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:18,927]\u001b[0m Trial 53 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.78973\n",
      "[1]\tvalidation-mae:254.53175\n",
      "[2]\tvalidation-mae:254.20924\n",
      "[3]\tvalidation-mae:253.92810\n",
      "[4]\tvalidation-mae:253.64925\n",
      "[5]\tvalidation-mae:253.33783\n",
      "[6]\tvalidation-mae:253.06648\n",
      "[7]\tvalidation-mae:252.79656\n",
      "[8]\tvalidation-mae:252.54997\n",
      "[9]\tvalidation-mae:252.30756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:19,372]\u001b[0m Trial 54 finished with value: 252.3433900164564 and parameters: {'booster': 'dart', 'colsample_bytree': 0.31620459621586505, 'eta': 0.0015960696780925882, 'gamma': 0.1321944574783282, 'max_depth': 4, 'max_leaves': 9, 'alpha': 8.49015536362924e-05, 'lambda': 1.1154510664532382e-08}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:19] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:196.07661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:19,441]\u001b[0m Trial 55 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:248.35158\n",
      "[1]\tvalidation-mae:242.57211\n",
      "[2]\tvalidation-mae:236.17700\n",
      "[3]\tvalidation-mae:229.58321\n",
      "[4]\tvalidation-mae:224.68851\n",
      "[5]\tvalidation-mae:219.75543\n",
      "[6]\tvalidation-mae:214.14856\n",
      "[7]\tvalidation-mae:208.68611\n",
      "[8]\tvalidation-mae:205.19319\n",
      "[9]\tvalidation-mae:202.09381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:19,688]\u001b[0m Trial 56 finished with value: 202.09325287986834 and parameters: {'booster': 'dart', 'colsample_bytree': 0.299040683224891, 'eta': 0.042590747006774315, 'gamma': 0.11617385133811146, 'max_depth': 4, 'max_leaves': 9, 'alpha': 7.417610578406884e-05, 'lambda': 1.6733567698478566e-08}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:241.10777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:19,746]\u001b[0m Trial 57 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:248.99297\n",
      "[1]\tvalidation-mae:243.68022\n",
      "[2]\tvalidation-mae:237.80005\n",
      "[3]\tvalidation-mae:231.79384\n",
      "[4]\tvalidation-mae:227.24655\n",
      "[5]\tvalidation-mae:222.62585\n",
      "[6]\tvalidation-mae:217.45244\n",
      "[7]\tvalidation-mae:212.40628\n",
      "[8]\tvalidation-mae:209.11961\n",
      "[9]\tvalidation-mae:206.17337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:20,025]\u001b[0m Trial 58 finished with value: 206.17059791552387 and parameters: {'booster': 'dart', 'colsample_bytree': 0.32668087613149976, 'eta': 0.03830217473948192, 'gamma': 0.5411765233736836, 'max_depth': 4, 'max_leaves': 8, 'alpha': 1.9882029975372926e-05, 'lambda': 4.944970349623823e-07}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:20,090]\u001b[0m Trial 59 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:219.52945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:20,143]\u001b[0m Trial 60 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.51030\n",
      "[1]\tvalidation-mae:253.96066\n",
      "[2]\tvalidation-mae:253.19075\n",
      "[3]\tvalidation-mae:252.59126\n",
      "[4]\tvalidation-mae:252.00435\n",
      "[5]\tvalidation-mae:251.34053\n",
      "[6]\tvalidation-mae:250.77394\n",
      "[7]\tvalidation-mae:250.24315\n",
      "[8]\tvalidation-mae:249.74136\n",
      "[9]\tvalidation-mae:249.22272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:20,318]\u001b[0m Trial 61 finished with value: 249.2408118486012 and parameters: {'booster': 'dart', 'colsample_bytree': 0.22198301847366228, 'eta': 0.003417870542497161, 'gamma': 0.6792264400247416, 'max_depth': 5, 'max_leaves': 27, 'alpha': 5.675058532303386e-05, 'lambda': 1.9043784123569962e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:20,374]\u001b[0m Trial 62 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:250.26860\n",
      "[1]\tvalidation-mae:245.93509\n",
      "[2]\tvalidation-mae:241.08491\n",
      "[3]\tvalidation-mae:236.27852\n",
      "[4]\tvalidation-mae:232.46400\n",
      "[5]\tvalidation-mae:228.55223\n",
      "[6]\tvalidation-mae:224.30664\n",
      "[7]\tvalidation-mae:220.13628\n",
      "[8]\tvalidation-mae:217.27704\n",
      "[9]\tvalidation-mae:214.71100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:20,556]\u001b[0m Trial 63 finished with value: 214.71585298957763 and parameters: {'booster': 'dart', 'colsample_bytree': 0.2752413971261749, 'eta': 0.029943476729097432, 'gamma': 0.9766749147387395, 'max_depth': 4, 'max_leaves': 15, 'alpha': 9.16427445077811e-06, 'lambda': 6.185305736823409e-08}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.09703\n",
      "[1]\tvalidation-mae:253.15389\n",
      "[2]\tvalidation-mae:252.20462\n",
      "[3]\tvalidation-mae:251.16602\n",
      "[4]\tvalidation-mae:250.19630\n",
      "[5]\tvalidation-mae:249.31161\n",
      "[6]\tvalidation-mae:248.29663\n",
      "[7]\tvalidation-mae:247.28731\n",
      "[8]\tvalidation-mae:246.41872\n",
      "[9]\tvalidation-mae:245.58530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:20,740]\u001b[0m Trial 64 finished with value: 245.66209544706527 and parameters: {'booster': 'dart', 'colsample_bytree': 0.35256092192268623, 'eta': 0.005983268668855632, 'gamma': 0.7848126455412182, 'max_depth': 1, 'max_leaves': 24, 'alpha': 0.00011220245484736253, 'lambda': 9.432055950410641e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:239.06473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:20,803]\u001b[0m Trial 65 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:20,858]\u001b[0m Trial 66 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:20,909]\u001b[0m Trial 67 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:20,963]\u001b[0m Trial 68 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:20] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:255.01582\n",
      "[1]\tvalidation-mae:254.98259\n",
      "[2]\tvalidation-mae:254.93446\n",
      "[3]\tvalidation-mae:254.90019\n",
      "[4]\tvalidation-mae:254.86713\n",
      "[5]\tvalidation-mae:254.83476\n",
      "[6]\tvalidation-mae:254.80167\n",
      "[7]\tvalidation-mae:254.76443\n",
      "[8]\tvalidation-mae:254.73009\n",
      "[9]\tvalidation-mae:254.69273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:21,444]\u001b[0m Trial 69 finished with value: 254.54854635216677 and parameters: {'booster': 'dart', 'colsample_bytree': 0.08861564273312203, 'eta': 0.00020921762485031758, 'gamma': 0.8676543901513166, 'max_depth': 2, 'max_leaves': 21, 'alpha': 0.0015203111852558867, 'lambda': 0.00034093995138481214}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:21,653]\u001b[0m Trial 70 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:247.62610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:21,716]\u001b[0m Trial 71 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:242.60553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:21,772]\u001b[0m Trial 72 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:253.87196\n",
      "[1]\tvalidation-mae:252.68893\n",
      "[2]\tvalidation-mae:251.04718\n",
      "[3]\tvalidation-mae:249.89676\n",
      "[4]\tvalidation-mae:248.82628\n",
      "[5]\tvalidation-mae:247.76218\n",
      "[6]\tvalidation-mae:246.73061\n",
      "[7]\tvalidation-mae:245.64977\n",
      "[8]\tvalidation-mae:244.65726\n",
      "[9]\tvalidation-mae:243.66246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:21,985]\u001b[0m Trial 73 finished with value: 243.62808557323095 and parameters: {'booster': 'dart', 'colsample_bytree': 0.12448689216328802, 'eta': 0.007401173896325847, 'gamma': 0.8300007980893844, 'max_depth': 3, 'max_leaves': 26, 'alpha': 0.00019353950529356172, 'lambda': 3.781575928701051e-08}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.91333\n",
      "[1]\tvalidation-mae:254.77925\n",
      "[2]\tvalidation-mae:254.58365\n",
      "[3]\tvalidation-mae:254.44302\n",
      "[4]\tvalidation-mae:254.30786\n",
      "[5]\tvalidation-mae:254.17491\n",
      "[6]\tvalidation-mae:254.04204\n",
      "[7]\tvalidation-mae:253.89105\n",
      "[8]\tvalidation-mae:253.75157\n",
      "[9]\tvalidation-mae:253.60112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:22,212]\u001b[0m Trial 74 finished with value: 253.54251234229292 and parameters: {'booster': 'dart', 'colsample_bytree': 0.1003486067619045, 'eta': 0.0008552323186043016, 'gamma': 0.8907446691653041, 'max_depth': 2, 'max_leaves': 19, 'alpha': 1.2552581818596519e-05, 'lambda': 0.7336114247424794}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:22,454]\u001b[0m Trial 75 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:22,511]\u001b[0m Trial 76 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:250.30260\n",
      "[1]\tvalidation-mae:246.10176\n",
      "[2]\tvalidation-mae:240.82663\n",
      "[3]\tvalidation-mae:237.24922\n",
      "[4]\tvalidation-mae:234.18564\n",
      "[5]\tvalidation-mae:231.12537\n",
      "[6]\tvalidation-mae:228.25653\n",
      "[7]\tvalidation-mae:225.42471\n",
      "[8]\tvalidation-mae:222.72707\n",
      "[9]\tvalidation-mae:220.24556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:22,753]\u001b[0m Trial 77 finished with value: 220.2951179374657 and parameters: {'booster': 'dart', 'colsample_bytree': 0.06420449797086442, 'eta': 0.030422841893103292, 'gamma': 0.9787289333046606, 'max_depth': 2, 'max_leaves': 11, 'alpha': 0.0003833153368193954, 'lambda': 0.46745882605695277}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:22,820]\u001b[0m Trial 78 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:22,884]\u001b[0m Trial 79 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:22,940]\u001b[0m Trial 80 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.37712\n",
      "[1]\tvalidation-mae:253.85425\n",
      "[2]\tvalidation-mae:253.10402\n",
      "[3]\tvalidation-mae:252.52477\n",
      "[4]\tvalidation-mae:251.97584\n",
      "[5]\tvalidation-mae:251.46654\n",
      "[6]\tvalidation-mae:250.96152\n",
      "[7]\tvalidation-mae:250.45235\n",
      "[8]\tvalidation-mae:249.94800\n",
      "[9]\tvalidation-mae:249.45825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:23,138]\u001b[0m Trial 81 finished with value: 249.3944048272079 and parameters: {'booster': 'dart', 'colsample_bytree': 0.1925658092142814, 'eta': 0.003307807157960549, 'gamma': 0.9375995470575963, 'max_depth': 1, 'max_leaves': 19, 'alpha': 1.7106809453648256e-05, 'lambda': 0.0013449843512536792}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:248.30780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:23,211]\u001b[0m Trial 82 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:23,274]\u001b[0m Trial 83 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:131.72147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:23,343]\u001b[0m Trial 84 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.82455\n",
      "[1]\tvalidation-mae:254.58891\n",
      "[2]\tvalidation-mae:254.33890\n",
      "[3]\tvalidation-mae:254.08171\n",
      "[4]\tvalidation-mae:253.78908\n",
      "[5]\tvalidation-mae:253.47905\n",
      "[6]\tvalidation-mae:253.23155\n",
      "[7]\tvalidation-mae:252.98482\n",
      "[8]\tvalidation-mae:252.76149\n",
      "[9]\tvalidation-mae:252.53920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:23,886]\u001b[0m Trial 85 finished with value: 252.53208996160174 and parameters: {'booster': 'dart', 'colsample_bytree': 0.4277398711473549, 'eta': 0.0014580665468084245, 'gamma': 0.6845639184142229, 'max_depth': 4, 'max_leaves': 17, 'alpha': 3.587571111463048e-05, 'lambda': 0.11532779214234959}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:23,949]\u001b[0m Trial 86 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:24,008]\u001b[0m Trial 87 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.24402\n",
      "[1]\tvalidation-mae:247.41641\n",
      "[2]\tvalidation-mae:243.09355\n",
      "[3]\tvalidation-mae:238.93068\n",
      "[4]\tvalidation-mae:235.22421\n",
      "[5]\tvalidation-mae:231.70041\n",
      "[6]\tvalidation-mae:228.08240\n",
      "[7]\tvalidation-mae:224.57599\n",
      "[8]\tvalidation-mae:222.01395\n",
      "[9]\tvalidation-mae:219.72600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:24,269]\u001b[0m Trial 88 finished with value: 219.73395501919913 and parameters: {'booster': 'dart', 'colsample_bytree': 0.354398804485546, 'eta': 0.025628222065115502, 'gamma': 0.644989038983552, 'max_depth': 3, 'max_leaves': 15, 'alpha': 2.4145031524906997e-05, 'lambda': 1.4731491096036629}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:210.10185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:24,324]\u001b[0m Trial 89 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:24,403]\u001b[0m Trial 90 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.56473\n",
      "[1]\tvalidation-mae:248.23714\n",
      "[2]\tvalidation-mae:244.05742\n",
      "[3]\tvalidation-mae:241.30081\n",
      "[4]\tvalidation-mae:238.86986\n",
      "[5]\tvalidation-mae:236.40875\n",
      "[6]\tvalidation-mae:234.04977\n",
      "[7]\tvalidation-mae:231.70447\n",
      "[8]\tvalidation-mae:229.49066\n",
      "[9]\tvalidation-mae:227.34477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:24,648]\u001b[0m Trial 91 finished with value: 227.34448710916072 and parameters: {'booster': 'dart', 'colsample_bytree': 0.09620242036228785, 'eta': 0.022090462928305903, 'gamma': 0.7636410703255894, 'max_depth': 4, 'max_leaves': 18, 'alpha': 3.5044773036780644e-05, 'lambda': 0.07345253766217455}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.86636\n",
      "[1]\tvalidation-mae:254.66981\n",
      "[2]\tvalidation-mae:254.42284\n",
      "[3]\tvalidation-mae:254.20857\n",
      "[4]\tvalidation-mae:253.97670\n",
      "[5]\tvalidation-mae:253.74182\n",
      "[6]\tvalidation-mae:253.54117\n",
      "[7]\tvalidation-mae:253.34122\n",
      "[8]\tvalidation-mae:253.15041\n",
      "[9]\tvalidation-mae:252.95926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:24,894]\u001b[0m Trial 92 finished with value: 252.97476686780033 and parameters: {'booster': 'dart', 'colsample_bytree': 0.3100780794029421, 'eta': 0.0012151847412789686, 'gamma': 0.8819817679418771, 'max_depth': 3, 'max_leaves': 7, 'alpha': 5.2090230532636584e-05, 'lambda': 1.1970293088870107e-06}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:24,958]\u001b[0m Trial 93 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:25,018]\u001b[0m Trial 94 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.72170\n",
      "[1]\tvalidation-mae:254.38165\n",
      "[2]\tvalidation-mae:254.01292\n",
      "[3]\tvalidation-mae:253.65472\n",
      "[4]\tvalidation-mae:253.09270\n",
      "[5]\tvalidation-mae:252.64436\n",
      "[6]\tvalidation-mae:252.28758\n",
      "[7]\tvalidation-mae:251.92976\n",
      "[8]\tvalidation-mae:251.60887\n",
      "[9]\tvalidation-mae:251.27783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:25,709]\u001b[0m Trial 95 finished with value: 251.27207899067471 and parameters: {'booster': 'dart', 'colsample_bytree': 0.4580221647225393, 'eta': 0.0021098996281908046, 'gamma': 0.9273486555668882, 'max_depth': 4, 'max_leaves': 1, 'alpha': 2.0353381002318646e-05, 'lambda': 0.0021972810003742594}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.77422\n",
      "[1]\tvalidation-mae:248.80173\n",
      "[2]\tvalidation-mae:245.13879\n",
      "[3]\tvalidation-mae:241.94640\n",
      "[4]\tvalidation-mae:236.84467\n",
      "[5]\tvalidation-mae:233.84103\n",
      "[6]\tvalidation-mae:230.84589\n",
      "[7]\tvalidation-mae:227.92970\n",
      "[8]\tvalidation-mae:225.66048\n",
      "[9]\tvalidation-mae:223.35033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:25,968]\u001b[0m Trial 96 finished with value: 223.3368074602304 and parameters: {'booster': 'dart', 'colsample_bytree': 0.46069194775356637, 'eta': 0.0206332637717322, 'gamma': 0.9198713448494473, 'max_depth': 3, 'max_leaves': 1, 'alpha': 9.086545547092868e-05, 'lambda': 0.0024098059325655045}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-08 10:11:26,075]\u001b[0m Trial 97 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.96417\n",
      "[1]\tvalidation-mae:254.88238\n",
      "[2]\tvalidation-mae:254.77470\n",
      "[3]\tvalidation-mae:254.68083\n",
      "[4]\tvalidation-mae:254.57413\n",
      "[5]\tvalidation-mae:254.47444\n",
      "[6]\tvalidation-mae:254.38649\n",
      "[7]\tvalidation-mae:254.29880\n",
      "[8]\tvalidation-mae:254.21503\n",
      "[9]\tvalidation-mae:254.13110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:26,317]\u001b[0m Trial 98 finished with value: 254.19418540866704 and parameters: {'booster': 'dart', 'colsample_bytree': 0.39051635215996094, 'eta': 0.0005299469431834816, 'gamma': 0.8933135693432322, 'max_depth': 3, 'max_leaves': 8, 'alpha': 1.4480209138302373e-05, 'lambda': 0.003044197173668727}. Best is trial 34 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:240.61038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-08 10:11:26,377]\u001b[0m Trial 99 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=34, values=[254.54854635216677], datetime_start=datetime.datetime(2022, 4, 8, 10, 11, 14, 890147), datetime_complete=datetime.datetime(2022, 4, 8, 10, 11, 15, 104541), params={'booster': 'dart', 'colsample_bytree': 0.4158466200732063, 'eta': 0.00025675381378648045, 'gamma': 0.9941895100340654, 'max_depth': 3, 'max_leaves': 12, 'alpha': 9.463978241232774e-07, 'lambda': 7.470523006997358e-07}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear', 'dart')), 'colsample_bytree': UniformDistribution(high=1.0, low=0.0), 'eta': UniformDistribution(high=1.0, low=1e-08), 'gamma': UniformDistribution(high=1.0, low=1e-08), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'max_leaves': IntUniformDistribution(high=50, low=0, step=1), 'alpha': LogUniformDistribution(high=2.0, low=1e-08), 'lambda': LogUniformDistribution(high=2.0, low=1e-08)}, user_attrs={}, system_attrs={}, intermediate_values={0: 255.007782, 1: 254.968048, 2: 254.915863, 3: 254.87027, 4: 254.818497, 5: 254.770233, 6: 254.727493, 7: 254.685104, 8: 254.644409, 9: 254.6035}, trial_id=34, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 254.54854635216677\n",
      "best params : {'booster': 'dart', 'colsample_bytree': 0.4158466200732063, 'eta': 0.00025675381378648045, 'gamma': 0.9941895100340654, 'max_depth': 3, 'max_leaves': 12, 'alpha': 9.463978241232774e-07, 'lambda': 7.470523006997358e-07}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(f'mae : {trial.value}')\n",
    "print('best params :', trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models \n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto', verbose=-1)\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7, verbose=-1)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56, verbose=-1)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56, verbose=-1)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7,  verbose=-1)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)],)\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model_2 = StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "\n",
    "liste_model = [xgb, model_lgbm, model_lgbm_2, model_lgbm_3, model_lgbm_4, VC, stack_model, stack_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\", \"verbose\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.986720359083927\n",
      "test data 0.8967965326020695\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=True, with_std=False), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_selected = make_pipeline(preprocessor,  xgb)\n",
    "model_selected.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_selected.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stacking\n",
      "r2 score train data 0.986720359083927\n",
      "r2 score test data 0.8967965326020695\n",
      "mean squared error 4793.1408218943025\n",
      "median abs error 32.69251251220703\n",
      "mean abs error 48.643447046910445\n"
     ]
    }
   ],
   "source": [
    "print(f'model stacking') \n",
    "\n",
    "\n",
    "print('r2 score train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('r2 score test data', model_selected.score(test_data_all_X, test_data_all_y))\n",
    "y_pred = model_selected.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ne pas touché ! Models saved avec les meilleurs score sans logarithme  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:16:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:16:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:16:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:16:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:16:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:16:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('maxabsscaler',\n",
       "                                                                   MaxAbsScaler())]),\n",
       "                                                  ['temp', 'atemp', 'humidity',\n",
       "                                                   'windspeed', 'day',\n",
       "                                                   'hour']),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler(with_mean=False)),\n",
       "                                                                  ('onehotencode...\n",
       "                                                             n_estimators=100,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             predictor=None,\n",
       "                                                             random_state=2,\n",
       "                                                             reg_alpha=0.22936408146810935,\n",
       "                                                             reg_lambda=0.03113496231279571,\n",
       "                                                             scale_pos_weight=None,\n",
       "                                                             subsample=None,\n",
       "                                                             subsample_freq=0.75,\n",
       "                                                             tree_method='auto',\n",
       "                                                             validate_parameters=None, ...)),\n",
       "                                               ('4',\n",
       "                                                LGBMRegressor(max_bin=1023,\n",
       "                                                              min_data_in_leaf=100,\n",
       "                                                              random_state=2,\n",
       "                                                              reg_alpha=0,\n",
       "                                                              subsample_freq=56))]))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meilleur model : train data 0.9534144126106945\n",
    "#                  test data 0.9222952589076324 ou 92.24 avec StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "# All models \n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gblinear', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=2, \n",
    "                            reg_alpha=0.22936408146810935, reg_lambda = 0.03113496231279571, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "\n",
    "xgb2 = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model_2 = StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', model_lgbm_3)])\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(SimpleImputer(), StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_selected = make_pipeline(preprocessor,  stack_model_2) # CHoisi le modele ici \n",
    "\n",
    "model_selected.fit(train_data_all_X , train_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model selectionnée\n",
      "r2 score train data 0.9534109580772161\n",
      "r2 score test data 0.9223042142455796\n",
      "mean squared error 3608.4721936012556\n",
      "median abs error 26.121466817633035\n",
      "mean abs error 40.76929353338032\n"
     ]
    }
   ],
   "source": [
    "print(f'model selectionnée') \n",
    "\n",
    "\n",
    "print('r2 score train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('r2 score test data', model_selected.score(test_data_all_X, test_data_all_y))\n",
    "y_pred = model_selected.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'stacking.pkl'\n",
    "# pickle.dump(model_selected, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45357ead6f4da03ac2d24128fc463b8aec036f33d9539f6445fb7785aa61cc64"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('envIA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
