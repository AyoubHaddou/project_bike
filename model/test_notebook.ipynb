{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDClassifier, Lasso,RidgeCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve, KFold\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_absolute_percentage_error, RocCurveDisplay, mean_squared_log_error,  accuracy_score,precision_score,recall_score,f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, RobustScaler, StandardScaler, FunctionTransformer, LabelEncoder, OneHotEncoder, Binarizer, OrdinalEncoder, MaxAbsScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, RFECV \n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor, ExtraTreesRegressor, VotingClassifier, VotingRegressor,StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# from pycaret.regression import *\n",
    "import xgboost\n",
    "from os import stat\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from contextlib import closing\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from datetime import datetime \n",
    "import lightgbm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle : \n",
    "\n",
    "https://www.kaggle.com/c/bike-sharing-demand/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions run_df avec pandas\n",
    "\n",
    "# # Data pour Lille à voir si utilisable  \n",
    "# # https://opendata.lillemetropole.fr/explore/dataset/vlille-realtime/table/\n",
    "\n",
    "# # Skip logs about pandas caveats documentations \n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "\n",
    "# # Pipeline pour la prédiction personnalisée : \n",
    "\n",
    "# def run_df_dc_personalised(df, date, time, weather, temp, temp_feel, humidity, windspeed):\n",
    "    \n",
    "#     # Day of year \n",
    "#     # print(time[0:2])\n",
    "    \n",
    "#     df.iloc[0:1,3]= str(weather)\n",
    "#     df.iloc[0:1,4] = int(temp)\n",
    "#     df.iloc[0:1,5]= int(temp_feel)\n",
    "#     df.iloc[0:1,6] = int(humidity)\n",
    "#     df.iloc[0:1,7]= int(windspeed)\n",
    "#     df.iloc[0:1,8]=  pd.to_datetime(str(date)).dayofyear   \n",
    "#     time = str(time) \n",
    "#     df.iloc[0:1,9] = int(time[0:2])\n",
    "    \n",
    "#     make_season_holiday_dc(df)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# # Pipeline pour la prediction automatique pour Washington DC : \n",
    "# def run_df_dc():\n",
    "        \n",
    "#     df_app = make_df_init(index=40)\n",
    "#     stat_api_3h(df_app, lat='38.8951', lon='77.0364')  \n",
    "#     make_season_holiday_dc(df_app)\n",
    "#     converstion_fr_dc(df_app)\n",
    "#     return df_app\n",
    "\n",
    "\n",
    "# # Pipeline pour la prediction automatique pour Lille : \n",
    "\n",
    "# def run_df_lille():\n",
    "    \n",
    "#     # Prendre la 1ere lignes du datafram pour le jour d'aujourd'hui\n",
    "#     df_app = make_df_init(index=40)\n",
    "#     stat_api_3h(df_app, lat='50.62925', lon='3.057256')\n",
    "#     make_season_holiday_fr(df_app)\n",
    "#     return df_app\n",
    "\n",
    "\n",
    "# def run_df_meteo_1h_lille():\n",
    "    \n",
    "#     # Prendre la 1ere lignes du datafram pour le jour d'aujourd'hui\n",
    "#     df_app = make_df_init(48)\n",
    "#     stat_api_1h(df_app, lat='50.62925', lon='3.057256')\n",
    "#     add_day_hour_from_nom(df_app)\n",
    "#     make_season_holiday_fr(df_app)\n",
    "#     return df_app\n",
    "\n",
    "\n",
    "# def run_df_meteo_1h_dc():\n",
    "    \n",
    "#     # Prendre la 1ere lignes du datafram pour le jour d'aujourd'hui\n",
    "#     df_app = make_df_init(48)\n",
    "#     stat_api_1h(df_app, lat='38.8951', lon='77.0364')   \n",
    "#     add_day_hour_from_nom(df_app)\n",
    "#     make_season_holiday_fr(df_app)\n",
    "#     converstion_fr_dc(df_app)\n",
    "#     return df_app\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pipelines fonctions avec pandas \n",
    "\n",
    "\n",
    "# # J'importe et je modifie le df de train pour en faire le df de prédiction de base avec les colonnes dans le bonnes ordres. \n",
    "# def make_df_init(index):\n",
    "#     df = pd.read_csv('./data/train.csv')\n",
    "#     df.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "#     df.drop('datetime', axis=1, inplace=True)\n",
    "#     df.drop(columns=['registered', 'casual'], index=1, inplace=True)\n",
    "#     df = pd.DataFrame(columns=df.drop('count', axis=1).keys(), index=np.arange(0,index,1))\n",
    "   \n",
    "#     return df \n",
    "\n",
    "# def stat_api_3h(df, lat, lon):\n",
    "#     date = []\n",
    "#     temp = []\n",
    "#     atemp = []\n",
    "#     humidity = []\n",
    "#     windspeed = []\n",
    "#     weather = []\n",
    "#     with closing(urlopen(f'http://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid=6bda77c15824913b0353424425dfeb64')) as f:\n",
    "#         cityEph = json.loads(f.read())\n",
    "#         for i in range(0, len(cityEph['list'])):\n",
    "#             temp.append(cityEph['list'][i]['main']['temp'])\n",
    "#             atemp.append(cityEph['list'][i]['main']['feels_like'])\n",
    "#             humidity.append(cityEph['list'][i]['main']['humidity'])\n",
    "#             windspeed.append(cityEph['list'][i]['wind']['speed'])\n",
    "#             weather.append(cityEph['list'][i]['weather'][0]['main'])\n",
    "#             date.append(cityEph['list'][i]['dt_txt'])\n",
    "            \n",
    "#     df['day'] = [ i for i in date]\n",
    "#     df['hour']  = pd.to_datetime(df['day']).dt.hour\n",
    "#     df['day']  = pd.to_datetime(df['day']).dt.dayofyear\n",
    "#     df['temp'] = [ i for i in temp]\n",
    "#     df['atemp'] = [ i for i in atemp]\n",
    "#     df['humidity'] = [ i for i in humidity]\n",
    "#     df['windspeed'] = [ i for i in windspeed]\n",
    "#     df['weather'] = [ str(i) for i in weather]\n",
    "#     df['temp'] = df['temp'] - 273.15\n",
    "#     df['atemp'] = df['atemp'] - 273.15\n",
    "    \n",
    "    \n",
    "# def stat_api_1h(df, lat, lon):\n",
    "#     date = []\n",
    "#     temp = []\n",
    "#     atemp = []\n",
    "#     humidity = []\n",
    "#     windspeed = []\n",
    "#     weather = []\n",
    "#     with closing(urlopen(f'https://api.openweathermap.org/data/2.5/onecall?lat={lat}&lon={lon}&exclude=minutely&appid=6bda77c15824913b0353424425dfeb64')) as f:\n",
    "#         cityEph = json.loads(f.read())    \n",
    "#         for i in range(0, len(cityEph['hourly'])): \n",
    "\n",
    "#             temp.append(cityEph['hourly'][i]['temp'])\n",
    "#             atemp.append(cityEph['hourly'][i]['feels_like'])\n",
    "#             humidity.append(cityEph['hourly'][i]['humidity'])\n",
    "#             windspeed.append(cityEph['hourly'][i]['wind_speed'])\n",
    "#             weather.append(cityEph['hourly'][i]['weather'][0]['main'])\n",
    "#             date.append(cityEph['hourly'][i]['dt'])\n",
    "            \n",
    "#                                 # hourly : 48h de data par heure  ; \n",
    "#                                 # current : actuel \n",
    "#                                 # daily : 8j \n",
    "    \n",
    "    \n",
    "#     df['temp'] = [ i for i in temp]\n",
    "#     df['atemp'] = [ i for i in atemp]\n",
    "#     df['humidity'] = [ i for i in humidity]\n",
    "#     df['windspeed'] = [ i for i in windspeed]\n",
    "#     df['weather'] = [ str(i) for i in weather]\n",
    "#     df['temp'] = df['temp'] - 273.15\n",
    "#     df['atemp'] = df['atemp'] - 273.15\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# def make_season_holiday_dc(df):\n",
    "\n",
    "#     # Season \n",
    "#     df['season'][(df['day'] >= 78 ) & (df['day'] < 141 )] = 1\n",
    "#     df['season'][(df['day'] >= 141) & (df['day'] < 365 ) ] = 2\n",
    "#     df['season'][(df['day'] >= 365) & (df['day'] < 356 ) ] = 3\n",
    "#     df['season'][(df['day'] >= 356) | (df['day'] < 78 ) ] = 4\n",
    "\n",
    "#     # Holliday For Washington \n",
    "#     df['holiday'] = 0 \n",
    "#     df['holiday'][(df['day'] >= 357) | (df['day'] <= 2 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 52) & (df['day'] <= 56 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 69) & (df['day'] <= 70 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 101) & (df['day'] <= 108 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 185) & (df['day'] <= 240 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 279) & (df['day'] <= 283 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 315) & (df['day'] <= 329 ) ] = 1\n",
    "\n",
    "#     # Férié for Washington \n",
    "#     df['holiday'][df['day'] == 17 ] = 1\n",
    "#     df['holiday'][df['day'] == 26 ] = 1\n",
    "#     df['holiday'][df['day'] == 150 ] = 1\n",
    "#     df['holiday'][df['day'] == 129 ] = 1\n",
    "#     df['holiday'][df['day'] == 305 ] = 1\n",
    "#     df['holiday'][df['day'] == 308 ] = 1\n",
    "        \n",
    "#     df['workingday'] = 0\n",
    "#     for i in np.arange(1,365,7):\n",
    "#         df['workingday'][(df['day'] == i) | (df['day'] == i + 1) ] = 1\n",
    "        \n",
    "            \n",
    "#     df.rename(columns = {'weather' : 'weather2'}, inplace = True)\n",
    "#     df.insert(4, 'weather', 0)\n",
    "    \n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_1_from_api, weather_choice_1_from_page.lower()])] = 1\n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_2_from_api, weather_choice_2_from_page.lower()])] = 2\n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_3_from_api, weather_choice_3_from_page.lower()])] = 3\n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_4_from_api, weather_choice_4_from_page.lower()])] = 4\n",
    "#     df.drop('weather2', axis=1, inplace=True)\n",
    "    \n",
    "#     df['weather'] = df['weather'].astype(int)\n",
    "#     df['season'] = df['season'].astype(int)\n",
    "\n",
    "    \n",
    "    \n",
    "# def make_season_holiday_fr(df):\n",
    "\n",
    "#     # just need columns day to run this functions and \n",
    "\n",
    "#     # Season \n",
    "#     df['season'][(df['day'] >= 78 ) & (df['day'] < 141 )] = 1\n",
    "#     df['season'][(df['day'] >= 141) & (df['day'] < 365 ) ] = 2\n",
    "#     df['season'][(df['day'] >= 365) & (df['day'] < 356 ) ] = 3\n",
    "#     df['season'][(df['day'] >= 356) | (df['day'] < 78 ) ] = 4\n",
    "\n",
    "\n",
    "#     # Holliday France - Lille\n",
    "#     df['holiday'] = 0 \n",
    "#     df['holiday'][(df['day'] >= 352) | (df['day'] < 3 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 36) & (df['day'] < 52 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 99) & (df['day'] < 115 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 145) & (df['day'] < 150 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 188) & (df['day'] < 244 ) ] = 1\n",
    "#     df['holiday'][(df['day'] >= 296) & (df['day'] < 312 ) ] = 1\n",
    "\n",
    "#     # Férié \n",
    "#     df['holiday'][df['day'] == 17 ] = 1\n",
    "#     df['holiday'][df['day'] == 26 ] = 1\n",
    "#     df['holiday'][df['day'] == 150 ] = 1\n",
    "#     df['holiday'][df['day'] == 129 ] = 1\n",
    "#     df['holiday'][df['day'] == 305 ] = 1\n",
    "#     df['holiday'][df['day'] == 308 ] = 1\n",
    "\n",
    "#     df['workingday'] = 0\n",
    "#     for i in np.arange(1,365,7):\n",
    "#         df['workingday'][(df['day'] == i) | (df['day'] == i + 1) ] = 1\n",
    "        \n",
    "        \n",
    "#     df.rename(columns = {'weather' : 'weather2'}, inplace = True)\n",
    "#     df.insert(4, 'weather', 0)\n",
    "        \n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_1_from_api, weather_choice_1_from_page.lower()])] = 1\n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_2_from_api, weather_choice_2_from_page.lower()])] = 2\n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_3_from_api, weather_choice_3_from_page.lower()])] = 3\n",
    "#     df['weather'][df['weather2'].str.lower().isin([weather_choice_4_from_api, weather_choice_4_from_page.lower()])] = 4\n",
    "#     df.drop('weather2', axis=1, inplace=True)\n",
    "    \n",
    "#     df['weather'] = df['weather'].astype(int)\n",
    "#     df['season'] = df['season'].astype(int)\n",
    "    \n",
    "    \n",
    "# def add_day_hour_from_nom(df):\n",
    "\n",
    "#     day = datetime.now()\n",
    "#     df['day'] = day                          \n",
    "#     df['hour']  = pd.to_datetime(df['day']).dt.hour\n",
    "#     df['day']  = pd.to_datetime(df['day']).dt.dayofyear\n",
    "    \n",
    "#     for i in range(0, len(df['hour'])):\n",
    "        \n",
    "#         df.iloc[i,9] += i \n",
    "#         if df.iloc[i,9] >= 24 :\n",
    "#             df.iloc[i,9] -= 24\n",
    "#             df.iloc[i,8] += 1\n",
    "#         else:\n",
    "#             df.iloc[i,9] += 1 \n",
    "    \n",
    "#     return df\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Decalage horaire : -5h \n",
    "# def converstion_fr_dc(df):\n",
    "    \n",
    "#         for i in range(0, len(df['hour'])):\n",
    "        \n",
    "#             if df.iloc[i,9] <= 4 :\n",
    "#                 df.iloc[i,9] += 19\n",
    "#             else:\n",
    "#                 df.iloc[i,9] -= 5\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "# # faire graph count vélo par jour des 20 jours précédents la requêtes + 10 des jours suivants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'model_all.sav'\n",
    "# pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model with pickle file \n",
    "# save_model(best,'lightgbm-location-velo')\n",
    "# Tuned_rf = tune_model(rf) \n",
    "# Tuned_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime('2011-10-30').dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Add new ROW\n",
    "# df=df.append({'Name' : 'Apple' , 'Price' : 23, 'Stock' : 'No'} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather(x):\n",
    "    if x == 1:\n",
    "        x = 4\n",
    "    elif x == 2:\n",
    "        x = 3\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_season(x):\n",
    "    if x == 1:\n",
    "        x = 3\n",
    "    elif x == 2:\n",
    "        x = 4\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all =  pd.read_csv('../data/train.csv')\n",
    "train_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-17 08:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 75, 'atemp' : 72, 'humidity' : 2, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-07-15 09:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : 96, 'atemp' : 88, 'humidity' : 2, 'windspeed' : 500.0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-10 11:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 51, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-12 19:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 4, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-08 18:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 9, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-24 17:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -110, 'atemp' : -108, 'humidity' : 25, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-25 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 69, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : -500, 'atemp' : -55, 'humidity' : 0, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-07-21 15:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 2, 'temp' : 75, 'atemp' : 72.5, 'humidity' : 41, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-04 13:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 55, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-05 14:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 25, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-28 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 39, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 09:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : -110, 'atemp' : -108, 'humidity' : 10, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-27 12:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 57, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-01-02 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -500, 'atemp' : -55, 'humidity' : 84, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "\n",
    "train_data_all['day'] = pd.to_datetime(train_data_all['datetime']).dt.dayofyear\n",
    "train_data_all['hour'] = pd.to_datetime(train_data_all['datetime']).dt.hour\n",
    "train_data_all['year'] = pd.to_datetime(train_data_all['datetime']).dt.year\n",
    "\n",
    "train_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "for i in range(0, len(train_data_all.iloc[:,0])) :\n",
    "    train_data_all.iloc[i,0] = transform_season(int(train_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(train_data_all.iloc[:,3])) :\n",
    "    train_data_all.iloc[i,3] = transform_weather(int(train_data_all.iloc[i,3] ))\n",
    "\n",
    "\n",
    "train_data_all = train_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "augmentation = np.linspace(0, (144 * 100 / 230) *  train_data_all.shape[0] , train_data_all.shape[0])\n",
    "\n",
    "# train_data_all['augmentation'] = 0 \n",
    "# train_data_all['augmentation'] = augmentation # Etape suivante : Il faut order par day avant de drop datetime pour que ce soit cohérent \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_all_X = train_data_all.drop('count', axis = 1)\n",
    "train_data_all_y = train_data_all['count']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data_all =  pd.read_csv('../data/test.csv')\n",
    "test_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data_all['day'] = pd.to_datetime(test_data_all['datetime']).dt.dayofyear\n",
    "test_data_all['hour'] = pd.to_datetime(test_data_all['datetime']).dt.hour\n",
    "test_data_all['year'] = pd.to_datetime(test_data_all['datetime']).dt.year\n",
    "test_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(0, len(test_data_all.iloc[:,0])) :\n",
    "    test_data_all.iloc[i,0] = transform_season(int(test_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(test_data_all.iloc[:,3])) :\n",
    "    test_data_all.iloc[i,3] = transform_weather(int(test_data_all.iloc[i,3] ))\n",
    "\n",
    "# augmentation = np.linspace(0, (144 * 100 / 230) *  test_data_all.shape[0] / 100 , test_data_all.shape[0])\n",
    "# test_data_all['augmentation'] = augmentation\n",
    "\n",
    "\n",
    "test_data_all = test_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "test_data_all_X = test_data_all.drop('count', axis = 1)\n",
    "test_data_all_y = test_data_all['count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"season\":3.0,\"holiday\":0.0,\"workingday\":0.0,\"weather\":4.0,\"temp\":9.84,\"atemp\":14.395,\"humidity\":81.0,\"windspeed\":0.0,\"count\":16.0,\"day\":1.0,\"hour\":0.0,\"year\":2011.0}'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data = train_data_all.iloc[0,:].to_json()\n",
    "data = json.dumps(data)\n",
    "eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.parse\n",
    "# import urllib.request\n",
    "\n",
    "\n",
    "\n",
    "# test_post = urllib.parse.urlencode({\"season\": 3.0 ,\"holiday\":0.0,\"workingday\":0.0,\"weather\":4.0,\"temp\":9.84,\"atemp\":14.395,\"humidity\":81.0,\"windspeed\":0.0,\"count\":16.0,\"day\":1.0,\"hour\":0.0,\"year\":2011.0})\n",
    "\n",
    "\n",
    "\n",
    "# url = 'http://127.0.0.1:8000/predict/xgboost/'\n",
    "\n",
    "\n",
    "# with urllib.request.urlopen(url, eval(test_post)) as f:\n",
    "#     print(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------------------------------------------- Casual and registered   \n",
    "# # ---------------------------------------------------------------- Train set => \n",
    "# data_train =  pd.read_csv('../data/train.csv')\n",
    "# data_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# data_train['day'] = pd.to_datetime(data_train['datetime']).dt.dayofyear\n",
    "# data_train['hour'] = pd.to_datetime(data_train['datetime']).dt.hour\n",
    "# data_train['year'] = pd.to_datetime(data_train['datetime']).dt.year\n",
    "# data_train.drop(['datetime','count'], axis=1, inplace=True)\n",
    "\n",
    "# for i in range(0, len(data_train.iloc[:,0])) :\n",
    "#     data_train.iloc[i,0] = transform_season(int(data_train.iloc[i,0] ))\n",
    "\n",
    "\n",
    "# casual_train = data_train.drop('registered', axis=1)\n",
    "# registered_train = data_train.drop('casual', axis=1)\n",
    "\n",
    "# casual_train_X = casual_train.drop('casual', axis = 1)\n",
    "# casual_train_y = casual_train['casual']\n",
    "# registered_train_X = registered_train.drop('registered', axis=1)\n",
    "# registered_train_y = registered_train['registered']\n",
    "\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------- Test set => \n",
    "\n",
    "# data_test =  pd.read_csv('../data/test.csv')\n",
    "# data_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# data_test['day'] = pd.to_datetime(data_test['datetime']).dt.dayofyear\n",
    "# data_test['hour'] = pd.to_datetime(data_test['datetime']).dt.hour\n",
    "# data_test['year'] = pd.to_datetime(data_test['datetime']).dt.year\n",
    "# data_test.drop(['datetime','count'], axis=1, inplace=True)\n",
    "\n",
    "# for i in range(0, len(data_test.iloc[:,0])) :\n",
    "#     data_test.iloc[i,0] = transform_season(int(data_test.iloc[i,0] ))\n",
    "\n",
    "\n",
    "# casual_test = data_test.drop('registered', axis=1)\n",
    "# registered_test = data_test.drop('casual', axis=1)\n",
    "\n",
    "# casual_test_X = casual_test.drop('casual', axis = 1)\n",
    "# casual_test_y = casual_test['casual']\n",
    "# registered_test_X = registered_test.drop('registered', axis=1)\n",
    "# registered_test_y = registered_test['registered']\n",
    "# casual_test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire catégories : Temps dangereux : 0 ou 1\n",
    "# Ajouter des lignes > 60° comme étant dangereux idem < à -50 \n",
    "# Ajouter les lignes associés aux conditions métérologiques extremes encodé par la colonne temps dangereux avec 0 count. \n",
    "\n",
    "# Faire un booléens : journée idéale : Entre 23 et 35 + windspeed entre 3 et 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Meilleur model xgb : \n",
    "# # Preprocessing et model xgboost :0.8949094854204845(8s)\n",
    "\n",
    "# logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "# numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "# encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "# numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "# encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "# preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "# xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.3, gamma=0,max_depth = 6, max_leaves=0, n_estimators =100, objective='reg:linear', random_state=32, reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "# model_xgboost = make_pipeline(preprocessor,  xgb)\n",
    "# model_xgboost.fit(train_data_all_X , train_data_all_y)\n",
    "# model_xgboost.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:37:21] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9056841627368329"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:linear', random_state=32, reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "model_xgboost = make_pipeline(preprocessor,  xgb)\n",
    "model_xgboost.fit(train_data_all_X , train_data_all_y)\n",
    "model_xgboost.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'xgboost.pkl'\n",
    "# pickle.dump(model_xgboost, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# savedmodel = open('./lgbm.pkl', 'rb')\n",
    "# model = pickle.load(savedmodel)\n",
    "# savedmodel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9190960274167893"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "model_lgbm_fit.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9118832634446215"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meilleur model n2 lgbm : \n",
    "# Preprocessing et model lgbm :0.9118832634446215 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "lgbm_2 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_2 = make_pipeline(preprocessor,  lgbm_2)\n",
    "model_lgbm_2.fit(train_data_all_X , train_data_all_y)\n",
    "model_lgbm_2.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9074659738499581"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colsample_bytree => Pourcentage de features utilisé pour chaque arbre réduit a 46% et resultat pas loin de 73% + modif learning rate \n",
    "# Preprocessing et model lgbm : 0.9064545094483808(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_3 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_3_fit = make_pipeline(preprocessor, model_lgbm_3)\n",
    "model_lgbm_3_fit.fit(train_data_all_X , train_data_all_y)\n",
    "model_lgbm_3_fit.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_2 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_3 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:linear', random_state=32, reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.917337454042215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.917337454042215 (2s4)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , lgbm_2), ('extratree' , model_lgbm), ('lgbm' , model_lgbm_3)])\n",
    "\n",
    "model_voting = make_pipeline(preprocessor, VC)\n",
    "\n",
    "model_voting.fit(train_data_all_X, train_data_all_y)\n",
    "model_voting.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_2 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, \n",
    "                                    reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_3 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, \n",
    "                                        reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, \n",
    "                            objective='reg:linear', random_state=32, reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, \n",
    "                            tree_method='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:37:30] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[09:37:32] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:37:33] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:37:34] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:37:34] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:37:35] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[09:37:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9221826676210114"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9221826676210114 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "other_column = ['season','weather','holiday']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, StandardScaler())\n",
    "encode_pipeline = make_pipeline(OneHotEncoder())\n",
    "other_pipeline = make_pipeline(OrdinalEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor( [('1' , model_lgbm),( '2', lgbm_2), ('3', xgb)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18.33853857,  17.4564071 ,  13.19756972, ..., -25.2166762 ,\n",
       "         7.95040603,   6.54500025])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stack.predict(train_data_all_X.iloc[:,0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model_stack.pkl'\n",
    "pickle.dump(model_stack, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stacking\n",
      "mean squared error 3614.117256724331\n",
      "median abs error 29.32062125970355\n",
      "mean abs error 42.72674689447487\n",
      "score r2 0.9221826676210114\n"
     ]
    }
   ],
   "source": [
    "print('model stacking')\n",
    "y_pred = model_stack.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))\n",
    "print('score r2', model_stack.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lgbm \n",
      "mean squared error 3757.4719475925376\n",
      "median abs error 25.486814391122323\n",
      "mean abs error 41.09404334027951\n",
      "score r2 0.9190960274167893\n"
     ]
    }
   ],
   "source": [
    "print('model lgbm')\n",
    "y_pred = model_lgbm_fit.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))\n",
    "print('score r2', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_all_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/test_notebook.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/test_notebook.ipynb#ch0000035?line=0'>1</a>\u001b[0m test_data_all_X\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_all_X' is not defined"
     ]
    }
   ],
   "source": [
    "test_data_all_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model xgboost\n",
      "mean squared error 4380.367260279583\n",
      "median abs error 30.701797485351562\n",
      "mean abs error 45.93158832581502\n",
      "score r2 0.9056841627368329\n"
     ]
    }
   ],
   "source": [
    "print('model xgboost') \n",
    "y_pred = model_xgboost.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))\n",
    "print('score r2', model_xgboost.score(test_data_all_X, test_data_all_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:06:39] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:06:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[13:06:43] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:06:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:06:44] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:06:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:06:45] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:06:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:06:46] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:06:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:06:47] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:06:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9203225064322619"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9221826676210114 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "other_column = ['season','weather','holiday']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, StandardScaler())\n",
    "encode_pipeline = make_pipeline(OneHotEncoder())\n",
    "other_pipeline = make_pipeline(OrdinalEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor( [('1' , model_lgbm),( '2', lgbm_2), ('3', xgb), ('4', VC)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:24] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:07:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[13:07:28] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:07:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:07:29] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:07:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:07:30] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:07:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:07:31] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:07:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:07:32] WARNING: ../src/objective/regression_obj.cu:188: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:07:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9074187430646383"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Preprocessing et model Votingmodel : 0.9221826676210114 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), StandardScaler())\n",
    "encode_pipeline = make_pipeline(SimpleImputer(), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor( [('1' , model_lgbm),( '2', lgbm_2), ('3', xgb), ('4', model_lgbm_3)], final_estimator=VC)\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a00c006436b821f608c703318a74ed0ce3a811dd7e5d1614f28a5307e338c8c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('api_work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
